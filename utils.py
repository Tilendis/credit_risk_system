# utils.py
import os
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import json
import pandas as pd
import numpy as np

# ==================== è®¾ç½®ä¸­æ–‡å­—ä½“ ==================== 
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def plot_model_comparison(results, save_path):
    """
    ç”Ÿæˆæ¨¡å‹å¯¹æ¯”å›¾ï¼ˆé™æ€ç‰ˆï¼Œé›¶ä¾èµ–ï¼Œ100%å…¼å®¹ï¼‰
    è‡ªåŠ¨è¿‡æ»¤ 'best_model' å’Œ 'best_auc' ç­‰éå­—å…¸æ•°æ®
    """
    # æ ¸å¿ƒä¿®å¤ï¼šåªä¿ç•™å­—å…¸ç±»å‹çš„æ¨¡å‹ç»“æœ
    models = {k: v for k, v in results.items() if isinstance(v, dict) and 'accuracy' in v}
    
    if not models:
        print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ç»“æœ")
        return None
    
    # æå–æŒ‡æ ‡æ•°æ®
    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
    data = []
    
    for model_name, metrics_dict in models.items():
        for metric in metrics:
            value = metrics_dict.get(metric, 0)
            data.append({
                'model': model_name,
                'metric': metric,
                'value': value
            })
    
    df_plot = pd.DataFrame(data)
    
    # ç”Ÿæˆé™æ€æŸ±çŠ¶å›¾ï¼ˆé¿å…plotlyæ‰€æœ‰é—®é¢˜ï¼‰
    static_path = save_path.replace('.html', '.png')
    plt.figure(figsize=(12, 8))
    sns.barplot(data=df_plot, x='metric', y='value', hue='model')
    plt.title('æ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=16, fontweight='bold')
    plt.ylabel('åˆ†æ•°', fontsize=12)
    plt.xlabel('è¯„ä¼°æŒ‡æ ‡', fontsize=12)
    plt.legend(title='æ¨¡å‹', bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig(static_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… æ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜: {static_path}")
    return static_path

def generate_report(results, report_dir):
    """
    ç”Ÿæˆç»¼åˆæŠ¥å‘Šï¼ˆå…¼å®¹æ··åˆç±»å‹ç»“æœï¼‰
    """
    print(f"\nğŸ“„ ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
    
    # åˆ›å»ºç›®å½•
    os.makedirs(report_dir, exist_ok=True)
    
    # ç”Ÿæˆæ¨¡å‹å¯¹æ¯”å›¾
    comparison_path = os.path.join(report_dir, "model_comparison.html")
    plot_model_comparison(results, comparison_path)
    
    # è¿‡æ»¤æ¨¡å‹æ•°æ®ï¼ˆç”¨äºè¡¨æ ¼ç”Ÿæˆï¼‰
    models = {k: v for k, v in results.items() if isinstance(v, dict) and 'accuracy' in v}
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    readme_path = os.path.join(report_dir, "summary.md")
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write("# æ¨¡å‹è®­ç»ƒæŠ¥å‘Š\n\n")
        
        # æœ€ä½³æ¨¡å‹ä¿¡æ¯
        if 'best_model' in results:
            f.write(f"## ğŸ† æœ€ä½³æ¨¡å‹\n\n")
            f.write(f"- **æ¨¡å‹åç§°**: `{results['best_model']}`\n")
            f.write(f"- **ROC AUC**: {results.get('best_auc', 'N/A'):.4f}\n\n")
        
        # æ¨¡å‹æ€§èƒ½è¡¨æ ¼
        if models:
            f.write("## ğŸ“Š æ¨¡å‹æ€§èƒ½æŒ‡æ ‡\n\n")
            f.write("| æ¨¡å‹ | Accuracy | Precision | Recall | F1 | ROC AUC |\n")
            f.write("|------|----------|-----------|--------|----|---------|\n")
            
            for model_name, metrics_dict in models.items():
                f.write(f"| {model_name} | ")
                f.write(f"{metrics_dict.get('accuracy', 0):.4f} | ")
                f.write(f"{metrics_dict.get('precision', 0):.4f} | ")
                f.write(f"{metrics_dict.get('recall', 0):.4f} | ")
                f.write(f"{metrics_dict.get('f1', 0):.4f} | ")
                f.write(f"{metrics_dict.get('roc_auc', 0):.4f} |\n")
            
            # æ··æ·†çŸ©é˜µ
            f.write("\n## ğŸ”¢ æ··æ·†çŸ©é˜µ\n\n")
            for model_name, metrics_dict in models.items():
                f.write(f"### {model_name}\n")
                f.write(f"```\n{metrics_dict.get('confusion_matrix', 'N/A')}\n```\n\n")
        
        # æ•°æ®ä¿¡æ¯
        f.write("## ğŸ“ è¾“å‡ºæ–‡ä»¶\n\n")
        f.write("- `models/` æ–‡ä»¶å¤¹ï¼šè®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶ï¼ˆ.joblibï¼‰\n")
        f.write("- `eda_plots/` æ–‡ä»¶å¤¹ï¼šæ•°æ®å¯è§†åŒ–å›¾è¡¨\n")
        f.write("- `model_comparison.png`ï¼šæ¨¡å‹æ€§èƒ½å¯¹æ¯”å›¾\n")
        f.write("- `feature_importance.csv`ï¼šç‰¹å¾é‡è¦æ€§æ’å\n")
    
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {readme_path}")

def plot_confusion_matrix(cm, model_name, save_dir):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µå›¾"""
    os.makedirs(save_dir, exist_ok=True)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)
    plt.title(f'æ··æ·†çŸ©é˜µ: {model_name}', fontsize=14, fontweight='bold')
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.tight_layout()
    
    save_path = os.path.join(save_dir, f"cm_{model_name}.png")
    plt.savefig(save_path, dpi=300)
    plt.close()
    print(f"âœ… æ··æ·†çŸ©é˜µå›¾: {save_path}")

def generate_feature_importance_plot(model_path, X_sample, save_dir):
    """ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾"""
    os.makedirs(save_dir, exist_ok=True)
    
    fi_path = os.path.join(os.path.dirname(model_path), "feature_importance.csv")
    if os.path.exists(fi_path):
        fi_df = pd.read_csv(fi_path).head(20)
        
        plt.figure(figsize=(10, 12))
        sns.barplot(data=fi_df, x='importance', y='feature')
        plt.title('å‰20ä¸ªé‡è¦ç‰¹å¾ï¼ˆéšæœºæ£®æ—ï¼‰', fontsize=16, fontweight='bold')
        plt.xlabel('é‡è¦æ€§')
        plt.tight_layout()
        
        save_path = os.path.join(save_dir, "feature_importance.png")
        plt.savefig(save_path, dpi=300)
        plt.close()
        print(f"âœ… ç‰¹å¾é‡è¦æ€§å›¾: {save_path}")
    else:
        print("âš ï¸ ç‰¹å¾é‡è¦æ€§æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")

# ==================== å¿«é€Ÿæµ‹è¯• ====================
if __name__ == '__main__':
    # æ¨¡æ‹Ÿç»“æœï¼ˆåŒ…å«éå­—å…¸æ•°æ®ï¼Œæµ‹è¯•è¿‡æ»¤é€»è¾‘ï¼‰
    mock_results = {
        'logistic_regression': {
            'accuracy': 0.72, 'precision': 0.52, 'recall': 0.73,
            'f1': 0.61, 'roc_auc': 0.78, 'confusion_matrix': [[80, 20], [10, 40]]
        },
        'random_forest': {
            'accuracy': 0.85, 'precision': 0.82, 'recall': 0.82,
            'f1': 0.82, 'roc_auc': 0.91, 'confusion_matrix': [[85, 15], [15, 35]]
        },
        'best_model': 'random_forest',  # éå­—å…¸æ•°æ®
        'best_auc': 0.91                 # éå­—å…¸æ•°æ®
    }
    
    print("æµ‹è¯•utilsæ¨¡å—ï¼ˆå«éå­—å…¸æ•°æ®è¿‡æ»¤ï¼‰...")
    generate_report(mock_results, "./test_reports")
    print("âœ… æµ‹è¯•å®Œæˆï¼è¯·æ£€æŸ¥ test_reports æ–‡ä»¶å¤¹")